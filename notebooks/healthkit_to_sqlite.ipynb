{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database - functions for data back-end / manipulations\n",
    "\n",
    "This is using an alternate approach:\n",
    "  - Export all of my Apple HealthFit data from the Health app to export.zip \n",
    "  - Converted this to a SQLite database using `healthfit-to-sqlite`\n",
    " \n",
    "  Queries can then against this database to build the cache file (or possibly a smaller custom SQLite file).\n",
    "\n",
    "### TODO\n",
    "* This is still a work in progress\n",
    "* Then cache this data - maybe try another (small) sqlite db for the caching (instead of Parquet/Feather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import pendulum\n",
    "from sqlite_utils import Database\n",
    "import reverse_geocoder as rg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting HealthKit data / creating SQLite DB\n",
    "\n",
    "First export HealthKit data using the Health app - select your profile icon from the top-right of the main screen and then select **Export All Health Data** (this can take some time to create the `export.zip` file).\n",
    "\n",
    "The archive can be converted to a SQLite database using the following command:\n",
    "\n",
    "`healthkit-to-sqlite export.zip healthkit_db.sqlite`\n",
    "\n",
    "which requires `healthkit-to-sqlite` to be installed (note it is one of the requirements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEALTHKIT_DATA_PATH = \"/Users/mjboothaus/icloud/Data/apple_health_export\"\n",
    "export_zip = Path(HEALTHKIT_DATA_PATH) / \"export.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls $HEALTHKIT_DATA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_healthkit_export_to_sqlite(export_zip):\n",
    "    zip_file = export_zip.as_posix()\n",
    "    if export_zip.exists() is False:\n",
    "        print(zip_file, \": not found\")\n",
    "        return None, f\"{zip_file}: not found\"\n",
    "    zip_file_date = pendulum.instance(\n",
    "        dt.datetime.fromtimestamp(export_zip.stat().st_ctime)\n",
    "    )\n",
    "\n",
    "    db_file = zip_file.replace(\"export.zip\", \"healthkit_db.sqlite\")\n",
    "    if Path(db_file).exists() is True:\n",
    "        Path(db_file).unlink()\n",
    "    sp_cmd = f\"healthkit-to-sqlite {zip_file} {db_file}\"\n",
    "    print(sp_cmd)\n",
    "    print(\n",
    "        \"---------------------------------------------------------------------------------------------\"\n",
    "    )\n",
    "    print(\n",
    "        \"Please wait: converting healthkit export.zip to sqlite database (takes just over a minute)...\"\n",
    "    )\n",
    "\n",
    "    sp = subprocess.Popen(sp_cmd, stdout=subprocess.PIPE, shell=True)\n",
    "    (sp_output, sp_err) = sp.communicate()\n",
    "\n",
    "    # This makes the wait possible\n",
    "    sp_status = sp.wait()\n",
    "\n",
    "    db_file_with_date = db_file.replace(\n",
    "        \".sqlite\", \"_\" + zip_file_date.to_date_string().replace(\"-\", \"_\") + \".sqlite\"\n",
    "    )\n",
    "\n",
    "    export_zip.rename(\n",
    "        zip_file.replace(\n",
    "            \".zip\", \"_\" + zip_file_date.to_date_string().replace(\"-\", \"_\") + \".zip\"\n",
    "        )\n",
    "    )\n",
    "    Path(db_file).rename(db_file_with_date)\n",
    "\n",
    "    return db_file_with_date, sp_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_sql_query_in_file(filename_dot_sql, conn, parse_dates, echo_query=False):\n",
    "\n",
    "    query_file = Path.cwd().parent / \"sql\" / filename_dot_sql\n",
    "\n",
    "    with open(query_file, \"r\") as query:\n",
    "        sql_text = query.read()\n",
    "        if echo_query is True:\n",
    "            print(sql_text)\n",
    "        df = pd.read_sql_query(sql_text, conn, parse_dates=parse_dates)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthkit-to-sqlite /Users/mjboothaus/icloud/Data/apple_health_export/export.zip /Users/mjboothaus/icloud/Data/apple_health_export/healthkit_db.sqlite\n",
      "---------------------------------------------------------------------------------------------\n",
      "Please wait: converting healthkit export.zip to sqlite database (takes just over a minute)...\n"
     ]
    }
   ],
   "source": [
    "db_file, output = convert_healthkit_export_to_sqlite(export_zip)\n",
    "try:\n",
    "    db = Database(db_file)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts_df = create_df_from_sql_query_in_file(\n",
    "    \"select_star_walking_workouts.sql\", db.conn, [\"startDate\", \"endDate\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to find the start point in each walk workout -- as date is not a date field in db not clear if sort by date and limit 1 will work\n",
    "# might need to import table to pandas convert types and then export to db before doing query. Else use sqlite_utils to change column types.\n",
    "\n",
    "start_point_df = create_df_from_sql_query_in_file(\n",
    "    \"select_start_point_workout.sql\", db.conn, [\"date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_point_df = create_df_from_sql_query_in_file(\n",
    "    \"select_finish_point_workout.sql\", db.conn, [\"date\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts_summary_df = start_point_df.merge(\n",
    "    finish_point_df, how=\"inner\", on=\"workout_id\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(latitude, longitude):\n",
    "    location = rg.search((latitude, longitude))\n",
    "    return [location[0][\"name\"], location[0][\"admin1\"], location[0][\"cc\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading formatted geocoded file...\n"
     ]
    }
   ],
   "source": [
    "workouts_summary_df[\"start_location\"] = workouts_summary_df.apply(\n",
    "    lambda row: get_location(\n",
    "        float(row[\"start_latitude\"]), float(row[\"start_longitude\"])\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts_summary_df[\"finish_location\"] = workouts_summary_df.apply(\n",
    "    lambda row: get_location(\n",
    "        float(row[\"finish_latitude\"]), float(row[\"finish_longitude\"])\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_elapsed_time_minutes(finish_datetime, start_datetime):\n",
    "    dt = pendulum.parse(finish_datetime) - pendulum.parse(start_datetime)\n",
    "    return float(dt.in_seconds() / 60 / 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts_summary_df[\"elapsed_time_hours\"] = workouts_summary_df.apply(\n",
    "    lambda row: calculate_elapsed_time_minutes(\n",
    "        row[\"finish_datetime\"], row[\"start_datetime\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts_summary_df[\"start_datetime\"] = workouts_summary_df[\"start_datetime\"].apply(\n",
    "    lambda dt: pendulum.parse(dt, tz=\"Australia/Sydney\").to_datetime_string()\n",
    ")  # TODO: Need to convert from UTC to Sydney local time?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts_summary_df = workouts_summary_df.merge(\n",
    "    workouts_df, how=\"inner\", on=\"workout_id\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts_summary_df[\"startDate\"] = workouts_summary_df[\"startDate\"].apply(\n",
    "    lambda dt: pendulum.instance(dt).to_datetime_string()\n",
    ")\n",
    "workouts_summary_df[\"endDate\"] = workouts_summary_df[\"endDate\"].apply(\n",
    "    lambda dt: pendulum.instance(dt).to_datetime_string()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts_summary_df.to_excel(\"../data/workouts_summary.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert start_point_df[\"workout_id\"].nunique() / len(start_point_df) == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_walk_stats(walk_data):\n",
    "    total_time = dt.timedelta(0)\n",
    "    total_distance = 0\n",
    "\n",
    "    for hike in walk_data:\n",
    "        total_time += hike.index.max()\n",
    "        # print(iHike+1, walk_date[iHike], hike.index.max(), hike['dist'].max() / 1e3)\n",
    "        total_distance += hike[\"dist\"].max()\n",
    "    total_distance /= 1e3\n",
    "\n",
    "    start_coord = walk_data[0][[\"lat\", \"lon\"]].iloc[0].tolist()\n",
    "    end_coord = walk_data[-1][[\"lat\", \"lon\"]].iloc[-1].tolist()\n",
    "    return total_time, total_distance, start_coord, end_coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_walk_cached_data_for_app(db_file, n_rows_used=5):\n",
    "    # read in all of the walks data and sample at an appropriate frequency and cache for faster use in the app\n",
    "    db = Database(db_file)\n",
    "    walk_df = pd.read_sql_query(\"SELECT * FROM walks\", db.conn)\n",
    "\n",
    "    UNUSED_COLUMNS = [\"dist\", \"speed\"]\n",
    "\n",
    "    walk_df.drop(UNUSED_COLUMNS, axis=1, inplace=True)\n",
    "    walk_df.dropna(inplace=True)  # TODO: Check why there are a few NaNs\n",
    "    walk_df = walk_df.iloc[::n_rows_used].reset_index()  # downsample\n",
    "\n",
    "    walk_df.to_feather(Path(db_file.as_posix().replace(\".db\", \".cache.feather\")))\n",
    "\n",
    "    return walk_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not working yet -- this is the alternate approach to using the individual .FIT files\n",
    "# walk_df = create_walk_cached_data_for_app(db_file, 10)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a53a55b0073614e9e0c431f4185a85688d811cf9f818579d72a21abdfe61484e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
