{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database - functions for data back-end / manipulations\n",
    "\n",
    "This is using an alternate approach:\n",
    "  - Export all of my Apple HealthFit data from the Health app to export.zip \n",
    "  - Converted this to a SQLite database using `healthfit-to-sqlite`\n",
    "  - Run various SQL queries to allow for producing a summary of (walk/hike) workouts\n",
    " \n",
    "  Queries can then against this database to build the cache file (or possibly a smaller custom SQLite file) as input into the walk mapping app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import pendulum\n",
    "from sqlite_utils import Database\n",
    "import reverse_geocoder as rg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting HealthKit data / creating SQLite DB\n",
    "\n",
    "First export HealthKit data using the Health app - select your profile icon from the top-right of the main screen and then select **Export All Health Data** (this can take some time to create the `export.zip` file).\n",
    "\n",
    "The archive can be converted to a SQLite database using the following command:\n",
    "\n",
    "`healthkit-to-sqlite export.zip healthkit_db.sqlite`\n",
    "\n",
    "which requires the `healthkit-to-sqlite` library to be installed (note it is one of the requirements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEALTHKIT_DATA_PATH = \"/Users/mjboothaus/icloud/Data/apple_health_export\"\n",
    "export_zip = Path(HEALTHKIT_DATA_PATH) / \"export.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mjboothaus/icloud/Data/apple_health_export/export.zip'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_zip.as_posix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(latitude, longitude):\n",
    "    location = rg.search((latitude, longitude))\n",
    "    return [location[0][\"name\"], location[0][\"admin1\"], location[0][\"cc\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_elapsed_time_minutes(finish_datetime, start_datetime):\n",
    "    dt = pendulum.parse(finish_datetime) - pendulum.parse(start_datetime)\n",
    "    return float(dt.in_seconds() / 60 / 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_healthkit_export_to_sqlite(export_zip):\n",
    "    zip_file = export_zip.as_posix()\n",
    "    if export_zip.exists() is False:\n",
    "        print(zip_file, \": not found\")\n",
    "        return None, f\"{zip_file}: not found\"\n",
    "    zip_file_date = pendulum.instance(\n",
    "        dt.datetime.fromtimestamp(export_zip.stat().st_ctime)\n",
    "    )\n",
    "\n",
    "    db_file = zip_file.replace(\"export.zip\", \"healthkit_db.sqlite\")\n",
    "    if Path(db_file).exists() is True:\n",
    "        Path(db_file).unlink()\n",
    "    sp_cmd = f\"healthkit-to-sqlite {zip_file} {db_file}\"\n",
    "\n",
    "    sp = subprocess.Popen(sp_cmd, stdout=subprocess.PIPE, shell=True)\n",
    "    (sp_output, _) = sp.communicate()\n",
    "\n",
    "    # This makes the wait possible\n",
    "    # sp_status = sp.wait()\n",
    "\n",
    "    db_file_with_date = db_file.replace(\n",
    "        \".sqlite\", \"_\" + zip_file_date.to_date_string().replace(\"-\", \"_\") + \".sqlite\"\n",
    "    )\n",
    "\n",
    "    export_zip.rename(\n",
    "        zip_file.replace(\n",
    "            \".zip\", \"_\" + zip_file_date.to_date_string().replace(\"-\", \"_\") + \".zip\"\n",
    "        )\n",
    "    )\n",
    "    Path(db_file).rename(db_file_with_date)\n",
    "\n",
    "    return db_file_with_date, sp_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_sql_query_in_file(\n",
    "    filename_dot_sql, conn, parse_dates, echo_query=False\n",
    "):\n",
    "\n",
    "    query_file = Path.cwd().parent / \"sql\" / filename_dot_sql\n",
    "\n",
    "    with open(query_file, \"r\") as query:\n",
    "        sql_text = query.read()\n",
    "        if echo_query is True:\n",
    "            print(sql_text)\n",
    "        df = pd.read_sql_query(sql_text, conn, parse_dates=parse_dates)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_walk_workout_summary(\n",
    "    db_file, output_file=\"../data/workouts_summary.csv\", include_location=False\n",
    "):\n",
    "    if db_file is None or Path(db_file).exists() is False:\n",
    "        print(\"SQLite database doesn't exist or not found\")\n",
    "        return None\n",
    "    db = Database(db_file)\n",
    "\n",
    "    # Extract data\n",
    "\n",
    "    workouts_df = create_df_from_sql_query_in_file(\n",
    "        \"select_star_walking_workouts.sql\", db.conn, [\"startDate\", \"endDate\"]\n",
    "    )\n",
    "    start_point_df = create_df_from_sql_query_in_file(\n",
    "        \"select_start_point_workout.sql\", db.conn, [\"date\"]\n",
    "    )\n",
    "    finish_point_df = create_df_from_sql_query_in_file(\n",
    "        \"select_finish_point_workout.sql\", db.conn, [\"date\"]\n",
    "    )\n",
    "\n",
    "    # Perform joins and additional column manipulations\n",
    "\n",
    "    workouts_df[\"startDate\"] = workouts_df[\"startDate\"].apply(\n",
    "        lambda dt: pendulum.instance(dt).to_datetime_string()\n",
    "    )\n",
    "    workouts_df[\"endDate\"] = workouts_df[\"endDate\"].apply(\n",
    "        lambda dt: pendulum.instance(dt).to_datetime_string()\n",
    "    )\n",
    "    workouts_summary_df = start_point_df.merge(\n",
    "        finish_point_df, how=\"inner\", on=\"workout_id\"\n",
    "    )\n",
    "    workouts_summary_df[\"elapsed_time_hours\"] = workouts_summary_df.apply(\n",
    "        lambda row: calculate_elapsed_time_minutes(\n",
    "            row[\"finish_datetime\"], row[\"start_datetime\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    workouts_summary_df[\"start_datetime\"] = workouts_summary_df[\"start_datetime\"].apply(\n",
    "        lambda dt: pendulum.parse(dt, tz=\"Australia/Sydney\").to_datetime_string()\n",
    "    )\n",
    "\n",
    "    if include_location is True:\n",
    "        workouts_summary_df[\"start_location\"] = workouts_summary_df.apply(\n",
    "            lambda row: get_location(\n",
    "                float(row[\"start_latitude\"]), float(row[\"start_longitude\"])\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        workouts_summary_df[\"finish_location\"] = workouts_summary_df.apply(\n",
    "            lambda row: get_location(\n",
    "                float(row[\"finish_latitude\"]), float(row[\"finish_longitude\"])\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "    workouts_summary_df = workouts_summary_df.merge(\n",
    "        workouts_df, how=\"inner\", on=\"workout_id\"\n",
    "    )\n",
    "\n",
    "    workouts_summary_df.to_csv(output_file, index=False)\n",
    "    return Path(output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_convert_create_walk_summary(path_export_zip, include_location=False):\n",
    "    db_file, _ = convert_healthkit_export_to_sqlite(path_export_zip)\n",
    "    output_file = create_walk_workout_summary(db_file, include_location=include_location)\n",
    "    return db_file, output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/mjboothaus/code/github/mjboothaus/emmaus-walking-data/.venv/bin/healthkit-to-sqlite\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/Users/mjboothaus/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/Users/mjboothaus/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/click/core.py\", line 1055, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/Users/mjboothaus/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/Users/mjboothaus/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/Users/mjboothaus/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/healthkit_to_sqlite/cli.py\", line 57, in cli\n",
      "    convert_xml_to_sqlite(fp, db, progress_callback=bar.update, zipfile=zf)\n",
      "  File \"/Users/mjboothaus/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/healthkit_to_sqlite/utils.py\", line 25, in convert_xml_to_sqlite\n",
      "    for tag, el in find_all_tags(\n",
      "  File \"/Users/mjboothaus/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/healthkit_to_sqlite/utils.py\", line 12, in find_all_tags\n",
      "    for event, el in parser.read_events():\n",
      "  File \"/Users/mjboothaus/.pyenv/versions/3.9.16/lib/python3.9/xml/etree/ElementTree.py\", line 1324, in read_events\n",
      "    raise event\n",
      "  File \"/Users/mjboothaus/.pyenv/versions/3.9.16/lib/python3.9/xml/etree/ElementTree.py\", line 1296, in feed\n",
      "    self._parser.feed(data)\n",
      "xml.etree.ElementTree.ParseError: syntax error: line 156, column 0\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'select\n    id as workout_id,\n    duration as duration_minutes,\n    totaldistance as totaldistance_km,\n    totalenergyburned as totalenergyburned_kJ,\n    sourcename,\n    sourceversion,\n    startdate,\n    enddate,\n    metadata_hkweathertemperature,\n    metadata_hkweatherhumidity,\n    metadata_hkelevationascended,\n    metadata_hkaveragemets\nfrom\n    workouts\nwhere workoutactivitytype = \"HKWorkoutActivityTypeWalking\" or workoutactivitytype = \"HKWorkoutActivityTypeHiking\" order by id\n\n\n /* Excluded fields:\n    workoutactivitytype,   # just walking\n    durationunit,          # fixed - min\n    totaldistanceunit,     # fixed - km\n    totalenergyburnedunit, # fixed - kJ\n    device,\n    creationdate,          # not really of interest (start date instead)\n    workout_events,        # think this is redundant info (need to check - JSON?)\n    metadata_hkgroupfitness,\n    metadata_hkworkoutbrandname,\n    metadata_hktimezone,\n    metadata_hkcoachedworkout,\n    metadata_hkwasuserentered,\n    metadata_hkindoorworkout,\n    metadata_hkelevationascended,\n    metadata_hkswimminglocationtype,\n    metadata_hkaveragemets,\n    metadata_healthfit_sub_sport,\n    metadata_healthfit_route,\n    metadata_healthfit_file_type,\n    metadata_hkmaximumspeed,\n    metadata_healthfit_total_moving_time,\n    metadata_healthfit_total_distance,\n    metadata_healthfit_max_running_cadence,\n    metadata_healthfit_min_altitude,\n    metadata_healthfit_avg_running_cadence,\n    metadata_healthfit_app_build,\n    metadata_healthfit_fit_sport,\n    metadata_healthfit_fit_sub_sport,\n    metadata_healthfit_max_altitude,\n    metadata_healthfit_fit_manufacturer,\n    metadata_healthfit_total_strides,\n    metadata_healthfit_fit_serial_number,\n    metadata_healthfit_sport,\n    metadata_hkaveragespeed,\n    metadata_healthfit_app_version,\n    metadata_hkexternaluuid\n */\n': no such table: workouts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/pandas/io/sql.py:2018\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2018\u001b[0m     cur\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2019\u001b[0m     \u001b[39mreturn\u001b[39;00m cur\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: workouts",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m db_file, output_file \u001b[39m=\u001b[39m main_convert_create_walk_summary(export_zip, include_location\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mmain_convert_create_walk_summary\u001b[0;34m(path_export_zip, include_location)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain_convert_create_walk_summary\u001b[39m(path_export_zip, include_location\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     db_file, _ \u001b[39m=\u001b[39m convert_healthkit_export_to_sqlite(path_export_zip)\n\u001b[0;32m----> 3\u001b[0m     output_file \u001b[39m=\u001b[39m create_walk_workout_summary(db_file, include_location\u001b[39m=\u001b[39;49minclude_location)\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m db_file, output_file\n",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m, in \u001b[0;36mcreate_walk_workout_summary\u001b[0;34m(db_file, output_file, include_location)\u001b[0m\n\u001b[1;32m      7\u001b[0m db \u001b[39m=\u001b[39m Database(db_file)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Extract data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m workouts_df \u001b[39m=\u001b[39m create_df_from_sql_query_in_file(\n\u001b[1;32m     12\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mselect_star_walking_workouts.sql\u001b[39;49m\u001b[39m\"\u001b[39;49m, db\u001b[39m.\u001b[39;49mconn, [\u001b[39m\"\u001b[39;49m\u001b[39mstartDate\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mendDate\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m start_point_df \u001b[39m=\u001b[39m create_df_from_sql_query_in_file(\n\u001b[1;32m     15\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mselect_start_point_workout.sql\u001b[39m\u001b[39m\"\u001b[39m, db\u001b[39m.\u001b[39mconn, [\u001b[39m\"\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m finish_point_df \u001b[39m=\u001b[39m create_df_from_sql_query_in_file(\n\u001b[1;32m     18\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mselect_finish_point_workout.sql\u001b[39m\u001b[39m\"\u001b[39m, db\u001b[39m.\u001b[39mconn, [\u001b[39m\"\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mcreate_df_from_sql_query_in_file\u001b[0;34m(filename_dot_sql, conn, parse_dates, echo_query)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m echo_query \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         \u001b[39mprint\u001b[39m(sql_text)\n\u001b[0;32m---> 11\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_sql_query(sql_text, conn, parse_dates\u001b[39m=\u001b[39;49mparse_dates)\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/pandas/io/sql.py:397\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39mRead SQL query into a DataFrame.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mparameter will be converted to UTC.\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    396\u001b[0m pandas_sql \u001b[39m=\u001b[39m pandasSQL_builder(con)\n\u001b[0;32m--> 397\u001b[0m \u001b[39mreturn\u001b[39;00m pandas_sql\u001b[39m.\u001b[39;49mread_query(\n\u001b[1;32m    398\u001b[0m     sql,\n\u001b[1;32m    399\u001b[0m     index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[1;32m    400\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    401\u001b[0m     coerce_float\u001b[39m=\u001b[39;49mcoerce_float,\n\u001b[1;32m    402\u001b[0m     parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[1;32m    403\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    404\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    405\u001b[0m )\n",
      "File \u001b[0;32m~/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/pandas/io/sql.py:2078\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_query\u001b[39m(\n\u001b[1;32m   2067\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2068\u001b[0m     sql,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m     dtype: DtypeArg \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2075\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m   2077\u001b[0m     args \u001b[39m=\u001b[39m _convert_params(sql, params)\n\u001b[0;32m-> 2078\u001b[0m     cursor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   2079\u001b[0m     columns \u001b[39m=\u001b[39m [col_desc[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m col_desc \u001b[39min\u001b[39;00m cursor\u001b[39m.\u001b[39mdescription]\n\u001b[1;32m   2081\u001b[0m     \u001b[39mif\u001b[39;00m chunksize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/github/mjboothaus/emmaus-walking-data/.venv/lib/python3.9/site-packages/pandas/io/sql.py:2030\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39minner_exc\u001b[39;00m\n\u001b[1;32m   2029\u001b[0m ex \u001b[39m=\u001b[39m DatabaseError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExecution failed on sql \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00margs[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mexc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2030\u001b[0m \u001b[39mraise\u001b[39;00m ex \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'select\n    id as workout_id,\n    duration as duration_minutes,\n    totaldistance as totaldistance_km,\n    totalenergyburned as totalenergyburned_kJ,\n    sourcename,\n    sourceversion,\n    startdate,\n    enddate,\n    metadata_hkweathertemperature,\n    metadata_hkweatherhumidity,\n    metadata_hkelevationascended,\n    metadata_hkaveragemets\nfrom\n    workouts\nwhere workoutactivitytype = \"HKWorkoutActivityTypeWalking\" or workoutactivitytype = \"HKWorkoutActivityTypeHiking\" order by id\n\n\n /* Excluded fields:\n    workoutactivitytype,   # just walking\n    durationunit,          # fixed - min\n    totaldistanceunit,     # fixed - km\n    totalenergyburnedunit, # fixed - kJ\n    device,\n    creationdate,          # not really of interest (start date instead)\n    workout_events,        # think this is redundant info (need to check - JSON?)\n    metadata_hkgroupfitness,\n    metadata_hkworkoutbrandname,\n    metadata_hktimezone,\n    metadata_hkcoachedworkout,\n    metadata_hkwasuserentered,\n    metadata_hkindoorworkout,\n    metadata_hkelevationascended,\n    metadata_hkswimminglocationtype,\n    metadata_hkaveragemets,\n    metadata_healthfit_sub_sport,\n    metadata_healthfit_route,\n    metadata_healthfit_file_type,\n    metadata_hkmaximumspeed,\n    metadata_healthfit_total_moving_time,\n    metadata_healthfit_total_distance,\n    metadata_healthfit_max_running_cadence,\n    metadata_healthfit_min_altitude,\n    metadata_healthfit_avg_running_cadence,\n    metadata_healthfit_app_build,\n    metadata_healthfit_fit_sport,\n    metadata_healthfit_fit_sub_sport,\n    metadata_healthfit_max_altitude,\n    metadata_healthfit_fit_manufacturer,\n    metadata_healthfit_total_strides,\n    metadata_healthfit_fit_serial_number,\n    metadata_healthfit_sport,\n    metadata_hkaveragespeed,\n    metadata_healthfit_app_version,\n    metadata_hkexternaluuid\n */\n': no such table: workouts"
     ]
    }
   ],
   "source": [
    "db_file, output_file = main_convert_create_walk_summary(export_zip, include_location=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_walk_stats(walk_data):\n",
    "    total_time = dt.timedelta(0)\n",
    "    total_distance = 0\n",
    "\n",
    "    for hike in walk_data:\n",
    "        total_time += hike.index.max()\n",
    "        # print(iHike+1, walk_date[iHike], hike.index.max(), hike['dist'].max() / 1e3)\n",
    "        total_distance += hike[\"dist\"].max()\n",
    "    total_distance /= 1e3\n",
    "\n",
    "    start_coord = walk_data[0][[\"lat\", \"lon\"]].iloc[0].tolist()\n",
    "    end_coord = walk_data[-1][[\"lat\", \"lon\"]].iloc[-1].tolist()\n",
    "    return total_time, total_distance, start_coord, end_coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_walk_cached_data_for_app(db_file, n_rows_used=5):\n",
    "    # read in all of the walks data and sample at an appropriate frequency and cache for faster use in the app\n",
    "    db = Database(db_file)\n",
    "    walk_df = pd.read_sql_query(\"SELECT * FROM walks\", db.conn)\n",
    "\n",
    "    UNUSED_COLUMNS = [\"dist\", \"speed\"]\n",
    "\n",
    "    walk_df.drop(UNUSED_COLUMNS, axis=1, inplace=True)\n",
    "    walk_df.dropna(inplace=True)  # TODO: Check why there are a few NaNs\n",
    "    walk_df = walk_df.iloc[::n_rows_used].reset_index()  # downsample\n",
    "\n",
    "    walk_df.to_feather(Path(db_file.as_posix().replace(\".db\", \".cache.feather\")))\n",
    "\n",
    "    return walk_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a53a55b0073614e9e0c431f4185a85688d811cf9f818579d72a21abdfe61484e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
